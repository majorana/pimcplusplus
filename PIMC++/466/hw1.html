<html > 
<head><title>Homework 1</title> 
<body bgcolor="#fff8e0" link=blue vlink=blue>

<h1 align=center style='text-align:center'>Homework 1:
Statistical Analysis of Data&nbsp; </h3>
<hr>

<h2>Question 1: Statistical Analysis of Data: Dataspork </h2>

<p><b>In this homework, you will familiarize yourself with the <code>Dataspork</code>*  
data analysis tool and use it to analyze a few data sets, sinilar to this output from simulation. </b>
<br>(*A "spork" is a combination fork and spoon, often given out at fastfood restaurants as a multipurpose. implement)
</p>

<p>For HW, you can use engineering workstations (EWS) remotely.</p>
<p>You can use access the EWS remotely by <code>ssh</code> (<i>secure shell</i> unix/linux protocal) 
to log-in remotely to Linux  machines located around engineering 
[e.g., Everitt Hall (eelnx#.ews.uiuc.edu), Digital Computing Lab (dcllnx#.ews.uiuc.edu), etc.]</p>

<p>at prompt&gt; &nbsp; <b>ssh dcllnx#.ews.uiuc.edu</b>   
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 
where <code>#</code> is machine number from 1-? (see EWS lab page for range). </p>

<p>Type <b><i>phys466</i></b> at the prompt. This will create a working directory for you.</p>

<p style="margin-top: 0pt;">You will be using a Data Analysis tool called <code>Dataspork </code>
to solve this homework. <br> You can read more about it&nbsp; 
<a href="http://mcc.uiuc.edu/dataspork" target="_new"> on the course web pages</a>.
<br>
[Note that when performing BLOCKING with data, the "blocking factor" axes is in terms
of 2^N (i.e., N=0,1,2,... for blocks of 1,2,4, etc.) and the "error" axes is 
sigma/sqrt(No. of Blocks).]  </p>

<p style="margin-top: 0pt;">To run it just open a web browser and go to the 
following link(be patient since it takes a little time to load):<font color="#0000ff"><u><a href="http://mccweb1.mcc.uiuc.edu/dataspork/demo.html"> &nbsp;http://mccweb1.mcc.uiuc.edu/dataspork/demo.html</a></u></font></p>

<p style="margin-top: 0pt;">An alternative way is to download it:
<a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/dataspork.1.1.jar">dataspork</a>. To run it from a <b>Unix</b> prompt type: <b>java -jar 
dataspork.1.1.jar</b> , otherwise, if you are on a <b>Windows</b> machine just 
click on the <b>open</b> tab after download is complete.. </p>

<p style="margin-top: 0pt;">Download the data files from here:

<br><a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/data1.txt">Dataset 1</a>

<br><a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/data2.txt">Dataset 2</a>

<br><a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/data3.txt">Dataset 3</a>

<br><a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/data4.txt">Dataset 4</a> </p>


<p></p> 1. From DataSpork's "File" menu, click "Open Dataset(s)".  
<br> 2. From the pull down menu on the right of the window that opens, select
"<b>AsciiDataReader</b>". 
<br> 3. Switch the "Files of type" option to "<b>All Files (*.*)</b>".  
<br> 4. Then open "data.1". A folder named "data.1" should appear in your window.
<br> 5. Under it will be "col 1".  <b>Right-click</b> on
"col 1" and then <b>left-click</b> "ScalarDataset".  
<br> The interface should be pretty intuitive from there.  <p></p>

<p>&nbsp;</p>

<ol start="1" type="1">
 <li>Data Set 1. (3 points) <br>
     The cutoffs should be automatically set to 0 and 999 (The first data point
     is numbered 0). The autocorrelation time should be nearly one. </li>
</ol>

<ul type="disc">
 <ul type="circle">
  <li> What is the mean value of this data? </li>
  <li> What is the error in the estimate of the mean? </li>
  <li> What is the square root of the variance (sigma) of the data? </li>
 </ul>
</ul>

<p style="margin-left: 1in;">Change the end cutoff to 500. </p>

<ul type="disc">
 <ul type="circle">
  <li> How does the error in the estimate of the mean change? </li>
  <li> How does sigma change?  </li>
  <li> How would you expect these quantities to change? </li>
 </ul>
</ul>

<ol start="2" type="1">
 <li>Data Set 2. (3 points)<br> This data set has correlations. </li>
</ol>

<ul type="disc">
 <ul type="circle">
  <li>What is the autocorrelation time of this series? </li>
  <li>What would be the error in the estimate of the mean without considering autocorrelation? </li>
  <li>Now what would be the error in the estimate of the mean with correlation? </li>
 </ul>
</ul>

<ol start="3" type="1">
 <li>Data Set 3. (3 points)<br> This data set has an initial transient. </li>
</ol>

<ul type="disc">
 <ul type="circle">
  <li>What is the mean (and its error) with the initial cutoff set to zero? </li>
  <li>Where should the initial cutoff be set? </li>
  <li>What is the mean (and its error) with the new cutoff? </li>
  <li> Is the difference between the mean in part-1 and part-3 significant? </li>
 </ul>
</ul>

<ol start="4" type="1">
 <li>Data Set 4. (5 points)
    <br> This data set was sampled from the distribution P(x) = b/(|x|<sup>a</sup>
     + c) with a = 2.2 and c = 1.0, b determined by the normalization.</li>
</ol>

<ul type="disc">
 <ul type="circle">
  <li> Based on this analytic expression, what do you expect for the mean? the variance? 
      <br> (Hint: for the variance, it's the behavior at large x that matters, so one can 
       use an approximation for the denominator of the distribution function. ) </li>
  <li> Look at the convergence of the mean and sigma by computing these values for five "end
      cutoffs" from 1000 to 5000 (i.e., 0-999, 0-1999, 0-2999, etc). 
      Do the same for data set 1 or 2 and compare the convergence behavior?
 </li></ul>
</ul>


<ol start="5"> 
<li><u>Comparison of data sets. (6 points)</u> 
<p> This Homework is to highlight the impact of Gauss' Central Limit Theorem, i.e.
</p><blockquote>
Given a population with a mean <i>m</i> and a finite, non-zero variance v = s<sup>2</sup>, 
the <b>sampling distribution of the mean approaches a normal distribution</b> with a mean of <i>m</i> and a variance of v<sub>m</sub> = s<sup>2</sup>/N  as N, the sample size, increases.
</blockquote>
<p> The words in bold are critical...the estimated mean approach a
gaussian distribution with more points used to estimate the mean, and
the sigma =sqrt[v<sub>m</sub>] is that needed for the normal
distribution of the mean. In this question, do the computations by hand
on your calculator, based upon standard <a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/lnotes/statfor.pdf"> statistical formulas (PDF)</a>.
</p><p> In debugging a code you have 2 versions A and B which you run 6 times
each to try to determine if they give the same answers.
you get the following answers:
</p></li></ol>

<center>
<table align="center" border="1" cellpadding="3" cellspacing="0">
 <tbody><tr align="center"> <td> A </td> <td> B </td> </tr>
 <tr align="center"> <td>1.12 </td> <td>1.44 </td> </tr>
 <tr align="center"> <td>1.52 </td> <td>1.34 </td> </tr>
 <tr align="center"> <td>1.33 </td> <td>1.19 </td> </tr>
 <tr align="center"> <td>1.09 </td> <td>1.13 </td> </tr>
 <tr align="center"> <td>1.20 </td> <td>1.56 </td> </tr>
 <tr align="center"> <td>1.26 </td> <td>1.45 </td> </tr>
</tbody></table>
</center>

<ul>
<ul>
<li> Compute the mean, variance, and the estimate of the error of the mean for 
A and B separately, assuming each run is <b>uncorrelated</b> with the others.

</li><li> Show that the probability that the two runs are (NOT) drawn from the same distribution is 71.1% (28.9%).  
<br>To do this, first find how many standard deviations (S.D.), x , 
the difference is from zero; do this by dividing the "estimate of the difference" by the 
"estimate of the error of the difference". From this number determine the probability 
the two are from the same distribution using a Normal Standard Probability Distribution 
Table (often referred to as P(0,x)) or the Complimentary Error Function (erfc(x/sqrt2)). 
<p>
<b>More straightforward  information on this can be found <a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/hw1_help/hw1_help.html">here</a>.</b>
While a basic overview of normal probability distributions is given in this help page, 
if you feel you need more, M. Boas, <i>Mathematical Methods in the Physical Sciences</i> has a good section on the subject. 

</p></li><li> Assuming that all the data is drawn from the same distribution, estimate 
the mean and the error of the mean of the combined data set.
</li></ul>
<p>
<b>Being able to perform these last two calculations is fundamental to using and understanding error bars 
based on the concept of normal probability distributions provided in link.</b>&nbsp;&nbsp;
</p><p>While a basic overview of normal probability distributions is given in the
help provided with the homework, if you feel you need more, see  M. Boas,
<i>Mathematical Methods in the Physical Sciences</i> has a good section on the subject.

</p></ul>

<ol start="6"> 
<li><u>Bias from unequilibrated data </u>. (6 points)
<br>Hint: This is a straight analytic application of the statistical formulas.

<p>In many-particle simulations there is often an equilibriation
period in which the initial configuration of the simulation biases the
output.  The data from this period should be discarded before
calculating statistics.  The following exercise is to study the effect
of neglecting to discard this data properly.

</p><p> The figure shows a plot of the trace of a scalar quantity (e.g.
energy) as a function of simulation time.  The interval [0,t<sub>1</sub>] 
is the unequilibrated period, in which the mean and variance are m<sub>1</sub>
and <img src="hw1_files/sigma.gif" border="0"><sub>1</sub><sup>2</sup>, respectively.  
The interval [t<sub>1</sub>,t<sub>2</sub>] is the good, equilibrated data, 
in which the mean and variance are m<sub>2</sub> and 
<img src="hw1_files/sigma.gif" border="0"><sub>2</sub><sup>2</sup>.
This mean is the true mean we are trying to calculate.

<br></p><center><img src="hw1_files/plot1.jpg" width="400"></center>
<p>
We imagine that we do not discard the first interval, as we should,
and now estimate the mean and variance for the whole interval 
[0, t<sub>2</sub>].  
We denote these quantities <b>m</b> and <img src="hw1_files/sigma.gif" border="0"><sup>2</sup>, respectively.

</p><ul> 
<li> Give an expression for <b>m</b>  as a function of 
<b>m<sub>1</sub></b>, <b>m<sub>2</sub></b>, <b>t<sub>1</sub></b> and <b>t<sub>2</sub></b>.  
Give also an expression of the systematic error, 
<b><img src="hw1_files/epsilon.gif" border="0"> = m - m<sub>2</sub></b>
that comes from including the bad data, in terms of the <i>prolongation</i> 
<b><img src="hw1_files/lambda.gif" border="0"> = t<sub>2</sub>/t<sub>1</sub></b> and 
<b><img src="hw1_files/Delta.gif" border="0"> =  m<sub>1</sub> - m<sub>2</sub></b>.

</li><li>Give an expression for  <img src="hw1_files/sigma.gif" border="0"><sup>2</sup>,
grouping your terms in inverse powers of <img src="hw1_files/lambda.gif" border="0">

</li><li> Consider t<sub>2</sub> &gt;&gt; t<sub>1</sub>, i.e. <img src="hw1_files/lambda.gif" border="0"> &gt;&gt; 1. 
At what time t<sub>2</sub> will the <i>statistical error of the mean</i> equal 
the <i>systematic error from the unequilibrated data</i>, 
utilizing the lowest nontrivial order of approximation.  
<br>(Don't forget the autocorrelation time, <img src="hw1_files/kappa.gif" border="0">).
With <img src="hw1_files/kappa.gif" border="0"> =1 and t<sub>1</sub> = 100, 
estimate this time by eyeballing the appropriate ratio from the plot.  
(You may wish to print this Figure and use a ruler to estimate. 
Print question and figure from  <a href="http://web.mse.uiuc.edu/courses/mse485/Homework/_hide_HW2007/hw1/prob.pdf"> PDF file</a>.)

</li></ul></li></ol>

<hr align="center" size="2" width="100%">


<h2>Question 2: Writing python code for statistical analysis</h2>
<p>
The elementary statistical analysis operations of obtaining the mean, standard deviation, autocorrelation time, and
error in the mean of a dataset are relatively straightforward to program. Although software such as Dataspork
exists to perform this analysis, you will have a deeper understanding of how to compute and interpret these
quantities after you have programmed them yourself. 
</p>

<p>
In this question, you will write a Python module that takes an array of scalar datapoints and computes the mean, 
standard  deviation, 
autocorrelation time, and estimated error in the mean. You will use the datasets provided in problem 1 to test 
your code and validate that your results agree with those of Dataspork. 
</p>

To start this question, please download the following files and put them in a directory: <br>
<a href="ReadData.py">ReadData.py</a><br>
<a href="CalcStatistics.py">CalcStatistics.py</a> <br>

You will work in this latter file (CalcStatistics.py)
Test that your python implementation is working correcly by calling <br>
<pre>
python CalcStatistics.py
</pre>

This file reads in a dataset for you from the file DataSet.txt. (you can change this name for your appropriate file
you are interested in reading)

<h3> 2.1 Mean</h3>

We provide example code for computing the mean of a one-dimensional array of floats.  If you are unfamiliar with
Python, use this block fo code to begin working on the problem:
<pre>
def mean(g):
  total=0.0
  num=0
  for i in range(0,len(g)):
    total+=g[i]
    num+=1
  return total/(num+0.0)
</pre>

Notice the following things:
<ul>
<li> Whitespace is important. You must indent underneat function calls and for loops (typically four spaces).  This
is somewhat unnatural and annoying but is designed to (and suceeds in) forcing code to be readable.
<li> To define a function use <code> def myFunctionName(param1,param2)</code>. Notice that there are no types in
teh parameter names. In fact, you never need to specify a type in python. 
</ul>

<h3> 2.2 Standard Deviation</h3>

<p>
You should review a textbook on elementary statistical analysis if you are unfamiliar with standard deviation. 
Typically, the standard deviation on the N-element dataset {O_i} is computed using this formula: <br>
FORMULAHERE , whcih si often written using "expectation value" notation as FORMULA. </p>
<p>
Implement this computation in your python module (you need only add an accumulator for the quantity O_i^2 and a
line of code to accumulate this quantity fo each data point.)  Change the name of your function to be stats and
have the last line return the result in a tuple. i.e.
<pre>
def stats(g):
   # do stuff here
   return (total/(num+0.0),st_dev)
</pre>

To use this function, you would call:
<pre>
  (myMean,myStdDev)=stats(myArray)
</pre>
</p>
<h3> 2.3 Autocorrelation time </h3>
<p>
Elementary statistical analysis assumes uncorrelated (i.e. statistically independent) data points in your set.  In
many practical cases, and especially in analyzing simulation data, correlations will be present that will cause you
to underestimate your error bars signifincatly unless they are treated correctly.  
</p>
<p>
Using a dataset {x_i} with N elements and mean xbar, the formula for the autocorrelation function follows:
FORMULA
</p>

<h3> 2.4 Standard Error </h3>
<p>
Although the standard deviation is indicative of the spread of the data, the standard error reflects the
fact that having more points gives better statistics.  It is the error you quote on your answer.
Having worked up to this point, you can easily compute this error, as well. 
You need the standard deviation \u03c3 and effective number of data points Neff = N- \u03ba. Then the error in the mean
is   ...
</p>
<p>
Your python module should now compute the mean, standard deviation, autocorrelation time, and error in a scalar
dataset (returned as a 4-tuple).
</p>

<p> 
Your should submit an electronic copy of your python functions as well as their result on being run on dataset4
</p>


<h2> Question 2: Periodic boundary conditions </h2>
3  Periodic Boundary Conditions

Often, the goal of atomic simulations is to measure bulk properties of a system. \u2018Bulk\u2019 means that one wishes to describe the many-body interactions of an infinite system (the so-called thermodynamic limit). A particle in a bulk system should interact with an infinite volume of particles around it; it cannot be at a surface or boundary, where it will experience different interactions.

Of course, we can simulate only a finite system on a computer, so we must make an approximation to the thermodynamic limit. Typically, the solution is to implement Periodic Boundary Conditions (PBC). This is like old video games where when you come out of one side of the screen, you enter on the other side.

The effect of PBC is that a particle in the simulation box \u2018sees\u2019 all other particles replicated on all sides of the simulation box, thus creating the effective interactions of a bulk system. A technical limitation of PBC is that there will be artificial spatial correlations on the order of the box size. However, it is a standard simulation technique and this artifact can be analyzed and mitigated by studying the effect of systematically increasing the system size.

It is important to ensure that you are enforcing PBC correctly; it can be tricky to get this right. You will implement PBC in a very basic molecule dynamics code.

3.1 Basic Molecular Dynamics in One Dimension

Start by representing a single particle in 1 dimension by the coordinate x=0.0; give it some velocity v=1.0 and choose a timestep dt=0.1. You will study the dynamics of this particle by writing a for loop that updates the particle\u2019s position using the relation from kinematics:

x(t + dt) = x(t) + v * dt.

Record the particle\u2019s position at each step in the loop (you should run through the loop N=100 times to start; you can adjust this value if you want to evolve the system for more or less time).

Run the code you have written so far. If you plot the particle\u2019s position as a function of simulation time, you should see that it moves linearly from x(0) = 0 to x(10) = 10.

3.2 PBC in One Dimension

Now, you want to confine the particle to a box of length L=2.0. Although the choice is arbitrary, it will be easier to center the simulation box at the origin, i.e. the simulation box is defined from x = -1 to x = 1. If the particle\u2019s position is outside the box, it should wrap around to the other boundary; this is the meaning of Periodic Boundary Conditions.

Implement this in your dynamics code. You can do so simply by checking the particle\u2019s position after every step in the for loop and resetting it to the equivalent position inside the simulation box if necessary.

Now, when you view the particle\u2019s position as a function of time, it should increase linearly until x = 1, whereupon the particle should wrap around to x = -1. The equivalent thing should happen if the particle moves \u201cto the left,\u201d e.g. if v=-1.0.

Think about how an implementation of PBC will work in three dimensions and with multiple particles.

3.3 Minimum Image Convention

The other prerequisite for performing bulk simulations using PBC is the minimum image convention. Take our one-dimensional example and add another particle, so that x1 = 0 and x2 = 0.5; imagine that the particles have a repulsive interaction so that the system will minimize its energy by maximizing the distance between particles. With open boundary conditions, particle 1 would simply move to the left (x1 \u2192-\u221e). The effect of PBC is to force particle 1 to interact with particle 2 at x2 = 0.5 and with its image at x2 = -1.5, so that particle 1 cannot minimize its energy by going to -\u221e.

The minimum image convention states that the interparticle distance for a pair of particles under PBC is the minimum distance for all pairs of images. For the one-dimensional case, this means r12 = |r2 - r1| will never be larger than L\u22152.

For x1 = 0 and x2 = 0.5, it should be obvious that r12 = 0.5.

For x1 = -0.5 and x2 = 0.5, it will be true that r12 = 1.0, which one can obtain naively, without worrying about the minimum image convention.

But let x1 = -0.6 and x2 = 0.5. Naively, you would compute r12 = 1.1, but the minimum image convention says that the minimum pair distance occurs between x1 = -0.6and the image of particle 2 at x2 = -1.5. Under the minimum image convention, then, the interparticle distance in this example is r12 = 0.9, not 1.1.

Once the concept is clear to you, implement this convention in your code by writing a dist function. Test it by placing a second particle at x2=0.5.

As you evolve the position of particle 1, compute the interparticle distance r12using the minimum image convention.
Verify that the value is always less then |L\u22152|. 

Again, think about generalizing your code to handle multiple particles and more than one dimension. 


</body>
</html> 



